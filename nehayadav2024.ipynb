{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":124863,"databundleVersionId":14712598,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================\n# UNIVERSAL KAGGLE ML PIPELINE (Classification)\n# Auto Model Selection + Imputer + Label Encode\n# 100% Stable for ANY Dataset\n# ============================================\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\n# Models\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# -----------------------------------------------\n# LOAD DATASETS\n# -----------------------------------------------\ntrain_df = pd.read_csv(\"/kaggle/input/mse-2-ai-201-b-ai-c/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/mse-2-ai-201-b-ai-c/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/mse-2-ai-201-b-ai-c/sample_submission.csv\")\n\nprint(\"Train shape:\", train_df.shape)\nprint(\"Test shape:\", test_df.shape)\nprint(\"Sample submission shape:\", sample_submission.shape)\n\n# -----------------------------------------------\n# TARGET (last column)\n# -----------------------------------------------\ntarget = train_df.columns[-1]\n\nX = train_df.drop(columns=[target])\n\n# Encode target\nle = LabelEncoder()\ny = le.fit_transform(train_df[target])\n\n# Test data\nX_test_final = test_df.copy()\n\n# -----------------------------------------------\n# TRAINâ€“VALIDATION SPLIT\n# -----------------------------------------------\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# -----------------------------------------------\n# DETECT COLUMN TYPES\n# -----------------------------------------------\ncat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\nnum_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n\n# -----------------------------------------------\n# PREPROCESSOR (Impute + OneHot + Scale)\n# -----------------------------------------------\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"cat\", Pipeline([\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n        ]), cat_cols),\n\n        (\"num\", Pipeline([\n            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n            (\"scaler\", StandardScaler())\n        ]), num_cols),\n    ]\n)\n\n# -----------------------------------------------\n# MODELS\n# -----------------------------------------------\nmodels = {\n    \"XGBoost\": XGBClassifier(\n        use_label_encoder=False,\n        eval_metric=\"logloss\",\n        n_estimators=400,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        n_jobs=-1\n    ),\n    \"LightGBM\": LGBMClassifier(\n        n_estimators=400,\n        learning_rate=0.05,\n        max_depth=-1,\n        n_jobs=-1\n    ),\n    \"CatBoost\": CatBoostClassifier(\n        iterations=500,\n        learning_rate=0.05,\n        depth=6,\n        verbose=0,\n        thread_count=-1\n    ),\n    \"RandomForest\": RandomForestClassifier(\n        n_estimators=400,\n        max_depth=None,\n        n_jobs=-1\n    ),\n    \"LogisticRegression\": LogisticRegression(\n        max_iter=2000,\n        n_jobs=-1\n    )\n}\n\n# -----------------------------------------------\n# MODEL TRAINING LOOP\n# -----------------------------------------------\nbest_model = None\nbest_logloss = np.inf\nbest_acc = 0.0\nbest_name = \"\"\n\nfor name, model in models.items():\n    print(f\"\\nTraining {name} ...\")\n\n    pipe = Pipeline([\n        (\"pre\", preprocess),\n        (\"model\", model)\n    ])\n\n    pipe.fit(X_train, y_train)\n\n    preds = pipe.predict(X_valid)\n    prob = pipe.predict_proba(X_valid)\n\n    acc = accuracy_score(y_valid, preds)\n    ll = log_loss(y_valid, prob)\n\n    print(f\"===== {name} =====\")\n    print(\"Accuracy:\", acc)\n    print(\"LogLoss :\", ll)\n\n    if ll < best_logloss:\n        best_logloss = ll\n        best_acc = acc\n        best_model = pipe\n        best_name = name\n\n# -----------------------------------------------\n# BEST MODEL\n# -----------------------------------------------\nprint(\"\\n===============================\")\nprint(\" BEST MODEL SELECTED AUTOMATICALLY \")\nprint(\"===============================\")\nprint(\"Model      :\", best_name)\nprint(\"Accuracy   :\", best_acc)\nprint(\"LogLoss    :\", best_logloss)\n\n# -----------------------------------------------\n# FINAL PREDICTIONS\n# -----------------------------------------------\nfinal_preds = best_model.predict(X_test_final)\n\n# -----------------------------------------------\n# SAFE SUBMISSION (matches test.csv rows)\n# -----------------------------------------------\nsubmission = pd.DataFrame()\n\n# Identify ID column for submission\nid_col = sample_submission.columns[0]\n\nif id_col in test_df.columns:\n    submission[id_col] = test_df[id_col]\nelse:\n    submission[id_col] = np.arange(len(test_df))\n\nsubmission[target] = le.inverse_transform(final_preds)\n\nsubmission.to_csv(\"submission_final.csv\", index=False)\nprint(\"\\nsubmission_final.csv saved!\")\nprint(submission.head())","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"OBI6tg5QhU6h","outputId":"56bc8a6f-357a-4197-dcb8-1e9819eda103","trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:33:58.967619Z","iopub.execute_input":"2025-12-17T19:33:58.968024Z","iopub.status.idle":"2025-12-17T19:34:25.150054Z","shell.execute_reply.started":"2025-12-17T19:33:58.967976Z","shell.execute_reply":"2025-12-17T19:34:25.148751Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py:195: SyntaxWarning: \"is not\" with 'tuple' literal. Did you mean \"!=\"?\n  if entities is not ():\n","output_type":"stream"},{"name":"stdout","text":"Train shape: (1824, 17)\nTest shape: (645, 17)\nSample submission shape: (450, 2)\n\nTraining XGBoost ...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [19:34:13] WARNING: /workspace/src/learner.cc:790: \nParameters: { \"use_label_encoder\" } are not used.\n\n  bst.update(dtrain, iteration=i, fobj=obj)\n","output_type":"stream"},{"name":"stdout","text":"===== XGBoost =====\nAccuracy: 0.8547945205479452\nLogLoss : 0.5283829286306402\n\nTraining LightGBM ...\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000569 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3628\n[LightGBM] [Info] Number of data points in the train set: 1459, number of used features: 16\n[LightGBM] [Info] Start training from score -0.666768\n[LightGBM] [Info] Start training from score -0.886912\n[LightGBM] [Info] Start training from score -2.594159\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"===== LightGBM =====\nAccuracy: 0.852054794520548\nLogLoss : 0.9165661294219809\n\nTraining CatBoost ...\n===== CatBoost =====\nAccuracy: 0.8575342465753425\nLogLoss : 0.4381735543881773\n\nTraining RandomForest ...\n===== RandomForest =====\nAccuracy: 0.8575342465753425\nLogLoss : 0.5348115623429283\n\nTraining LogisticRegression ...\n===== LogisticRegression =====\nAccuracy: 0.8054794520547945\nLogLoss : 0.5516521802916667\n\n===============================\n BEST MODEL SELECTED AUTOMATICALLY \n===============================\nModel      : CatBoost\nAccuracy   : 0.8575342465753425\nLogLoss    : 0.4381735543881773\n\nsubmission_final.csv saved!\n   id              Class\n0   1  Kirmizi_Pistachio\n1   2     Siit_Pistachio\n2   3  Kirmizi_Pistachio\n3   4  Kirmizi_Pistachio\n4   5  Kirmizi_Pistachio\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/preprocessing/_label.py:151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n","output_type":"stream"}],"execution_count":1}]}